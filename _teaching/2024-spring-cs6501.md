---
title: "CS 6501 (Spring 2024)"
collection: teaching
permalink: /teaching/2024-spring-cs6501
---



## Schedule

<style>
  table {
    font-size: 14px; /* Set your desired font size */
  }
  th, td {
    padding: 5px; /* Optional: Adjust padding for cells */
  }
</style>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Topic</th>
      <th>Papers</th>
      <th>Slides</th>
      <th>Recommended Reading</th>
    </tr>
  </thead>
  <tr>
      <td colspan="5" align="center"><b>Introduction to Language Models</b></td>
  </tr>
  <tbody>
    <tr>
      <td>1/17</td>
      <td>Course Overview</td>
      <td>-</td>
      <td>overview</td>
      <td>-</td>
    </tr>
    <tr>
      <td>1/22</td>
      <td>Language Model Architecture and Pretraining</td>
      <td>
        * <a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality (word2vec) </a><br>
        * <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (Transformer)</a><br>
        * <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners (GPT-2)</a> <br>
        * <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
        * <a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a><br> 
        * <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5) </a> <br> </td>
      <td>lm_basics</td>
    </tr>
    <tr>
      <td>1/24</td>
      <td> Large Language Models and In-Context Learning </td>
      <td>
        * <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners (GPT-3) </a><br>
        * <a href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models </a><br>
        * <a href="https://arxiv.org/abs/2111.02080">An Explanation of In-context Learning as Implicit Bayesian Inference </a><br>
        * <a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? </a><br>
      </td>
      <td>  </td>
    </tr>
    <tr>
      <td>1/29</td>
      <td> Scaling and Emergent Ability </td>
      <td>
        * <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2305.16264">Scaling Data-Constrained Language Models</a><br>
        * <a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a><br>  
      </td>
      <td>  </td>
    </tr>
  </tbody>
  
  
  <tr>
      <td colspan="5" align="center"><b>Reasoning with Language Models</b></td>
  </tr>
  <tbody>
  <tr>
      <td></td>
      <td> Chain-of-Thought Generation </td>
      <td>
        * <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models </a><br>
        * <a href="https://arxiv.org/abs/2210.11610">Large Language Models Can Self-Improve </a><br>
         </td>
      <td> </td>
  </tr>
  </tbody>
  
  <tr>
      <td colspan="5" align="center"><b>Language Model Alignment</b></td>
  </tr>
  
  
  <tr>
      <td colspan="5" align="center"><b>Retrieval-Agumented Language Generation</b></td>
  </tr>
  
  
</table>

