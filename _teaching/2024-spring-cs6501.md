---
title: "CS 6501 (Spring 2024)"
collection: teaching
permalink: /teaching/2024-spring-cs6501
---



## Schedule

<style>
  table {
    font-size: 14px; /* Set your desired font size */
    width: 100%; /* Set your desired width */
    border-collapse: collapse; /* Optional: Remove cell spacing */
  }
  th, td {
    padding: 5px; /* Optional: Adjust padding for cells */
  }
</style>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Topic</th>
      <th>Papers</th>
      <th>Slides</th>
      <th>Recommended Reading</th>
    </tr>
  </thead>
  <tr>
      <td colspan="5" align="center"><b>Introduction to Language Models</b></td>
  </tr>
  <tbody>
    <tr>
      <td>1/17</td>
      <td>Course Overview</td>
      <td>-</td>
      <td>overview</td>
      <td>-</td>
    </tr>
    <tr>
      <td>1/22</td>
      <td>Language Model Architecture and Pretraining</td>
      <td>
        * <a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality (word2vec) </a><br>
        * <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (Transformer)</a><br>
        * <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners (GPT-2)</a> <br>
        * <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
        * <a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a><br> 
        * <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5) </a> <br> </td>
      <td>lm_basics</td>
      <td> 
        * <a href="https://jalammar.github.io/illustrated-transformer/">(Blog) The Illustrated Transformer</a><br> 
        * <a href="https://kipp.ly/transformer-inference-arithmetic/">(Blog) Transformer Inference Arithmetic</a><br> </td>
    </tr>
    <tr>
      <td>1/24</td>
      <td> Large Language Models and In-Context Learning </td>
      <td>
        * <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners (GPT-3) </a><br>
        * <a href="https://arxiv.org/abs/2307.09288">Llama 2: Open Foundation and Fine-Tuned Chat Models </a><br>
        * <a href="https://arxiv.org/abs/2111.02080">An Explanation of In-context Learning as Implicit Bayesian Inference </a><br>
        * <a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? </a><br>
      </td>
      <td>  </td>
      <td> <a href="https://www.interconnects.ai/p/llama-2-from-meta">(Blog) Llama 2: an incredible open LLM</a><br> </td>
    </tr>
    <tr>
      <td>1/29</td>
      <td> Scaling and Emergent Ability </td>
      <td>
        * <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2305.16264">Scaling Data-Constrained Language Models</a><br>
        * <a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a><br>  
      </td>
      <td>  </td>
      <td>  </td>
    </tr>
  </tbody>
  
  
  <tr>
      <td colspan="5" align="center"><b>Reasoning with Language Models</b></td>
  </tr>
  <tbody>
  <tr>
      <td> date </td>
      <td> Chain-of-Thought Generation </td>
      <td>
        * <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models </a><br>
        * <a href="https://arxiv.org/abs/2210.11610">Large Language Models Can Self-Improve </a><br>
      </td>
      <td> </td>
  </tr>
  <tr>
      <td> date </td>
      <td> Advanced Reasoning </td>
      <td>
        * <a href="https://arxiv.org/abs/2211.10435">PAL: Program-aided Language Models </a><br>
        * <a href="https://arxiv.org/abs/2305.20050">Let's Verify Step by Step </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  </tbody>
  
  
  <tr>
      <td colspan="5" align="center"><b>Language Model Alignment</b></td>
  </tr>
  <tbody>
  <tr>
      <td> date </td>
      <td> Multi-Task Instruction Tuning </td>
      <td>
        * <a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners </a><br>
        * <a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization </a><br>
        * <a href="https://arxiv.org/abs/2104.08773">Cross-Task Generalization via Natural Language Crowdsourcing Instructions </a><br>
        * <a href="https://arxiv.org/abs/2204.07705">Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks </a><br>
      </td>
      <td> </td>
      <td> <a href="https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2">(Blog) A Stage Review of Instruction Tuning</a><br> </td>
  </tr>
  <tr>
      <td> date </td>
      <td> Chat-Based Instruction Tuning </td>
      <td>
        * <a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions </a><br>
        * <a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment </a><br>
        * <a href="https://arxiv.org/abs/2305.14387">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback </a><br>
        * <a href="https://arxiv.org/abs/2311.10702">Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2 </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  <tr>
      <td> date </td>
      <td> Reinforcement Learning from Human Feedback (RLHF) </td>
      <td>
        * <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback </a><br>
        * <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model </a><br>
        * <a href="https://arxiv.org/abs/2306.01693">Fine-Grained Human Feedback Gives Better Rewards for Language Model Training </a><br>
        * <a href="https://arxiv.org/abs/2307.15217">Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  </tbody>
  
  
  <tr>
      <td colspan="5" align="center"><b>Knowledge and Factuality</b></td>
  </tr>
  <tbody>
  <tr>
      <td> date </td>
      <td> Parametric Knowledge in Language Models </td>
      <td>
        * <a href="https://arxiv.org/abs/1909.01066">Language Models as Knowledge Bases? </a><br>
        * <a href="https://arxiv.org/abs/2002.08910">How Much Knowledge Can You Pack Into the Parameters of a Language Model? </a><br>
        * <a href="https://arxiv.org/abs/2012.14913">Transformer Feed-Forward Layers Are Key-Value Memories </a><br>
        * <a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  <tr>
      <td> date </td>
      <td> Retrieval-Agumented Language Generation (RAG) </td>
      <td>
        * <a href="https://arxiv.org/abs/1911.00172">Generalization through Memorization: Nearest Neighbor Language Models </a><br>
        * <a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks </a><br>
        * <a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering </a><br>
        * <a href="https://arxiv.org/abs/2112.04426">Improving language models by retrieving from trillions of tokens </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  <tr>
      <td> date </td>
      <td> Advanced RAG </td>
      <td>
        * <a href="https://arxiv.org/abs/2301.12652">REPLUG: Retrieval-Augmented Black-Box Language Models </a><br>
        * <a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts </a><br>
        * <a href="https://arxiv.org/abs/2212.14024">Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP </a><br>
        * <a href="https://arxiv.org/abs/2310.11511">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  </tbody>
  
  
  <tr>
      <td colspan="5" align="center"><b>Efficiency in Language Models</b></td>
  </tr>
  <tbody>
  <tr>
      <td> date </td>
      <td> Training Efficiency </td>
      <td>
        * <a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models </a><br>
        * <a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs </a><br>
        * <a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness </a><br>
        * <a href="https://arxiv.org/abs/2304.11277">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  <tr>
      <td> date </td>
      <td> Sparse Models </td>
      <td>
        * <a href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity </a><br>
        * <a href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer </a><br>
        * <a href="https://arxiv.org/abs/2309.17453">Efficient Streaming Language Models with Attention Sinks </a><br>
        * <a href="https://arxiv.org/abs/2301.00774">SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  <tr>
      <td> date </td>
      <td> Decoding Efficiency </td>
      <td>
        * <a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding </a><br>
        * <a href="https://arxiv.org/abs/2305.09781">SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification </a><br>
        * <a href="https://arxiv.org/abs/2307.15337">Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding </a><br>
        * <a href="https://arxiv.org/abs/2210.15097">Contrastive Decoding: Open-ended Text Generation as Optimization </a><br>
      </td>
      <td> </td>
      <td> </td>
  </tr>
  </tbody>
  
</table>

