---
title: "CS 6770 Natural Language Processing (Spring 2026)"
collection: teaching
permalink: /teaching/2026-spring-cs6770
layout: archive
---

<div class="course-page" markdown="1">

## Logistics
* Instructor: [**Yu Meng**](https://yumeng5.github.io/) (yumeng5[at]virginia[dot]edu)
* Teaching Assistants:
  * [**Wei-Lin Chen**](https://wlchen0206.github.io/) (wlchen[at]virginia[dot]edu)
  * [**Zhepei Wei**](https://www.cs.virginia.edu/~tqf5qb/) (zhepei.wei[at]virginia[dot]edu)
  * [**Xinyu Zhu**](https://zhuxinyu.top/) (xinyuzhu[at]virginia[dot]edu)
* Time: Mondays & Wednesdays 2:00pm - 3:15pm
* Office Hours: Mondays & Wednesdays after class 
* Location: Olsson Hall 009

## Course Overview
This advanced graduate-level course offers a comprehensive exploration of cutting-edge developments in the field of natural language processing (NLP). With Large Language Models (LLMs) serving as the foundation for state-of-the-art NLP systems, we will cover various topics aiming at gaining a better understanding of LLMs' design, capabilities, limitations, and future prospects. Key areas include model architecture and design, training methodologies (e.g., pretraining, instruction tuning, RLHF, RLVR), emergent capabilities (e.g., in-context learning, reasoning), parametric knowledge with retrieval-augmented generation, efficiency (e.g., parameter-efficient training, sparse methods), language agents (e.g., multimodality, computer use), and ethics. This course will be highly research-driven with a substantial focus on reading, presenting and discussing important papers and conducting research projects.

## Grading

* **Paper Presentation (30%)** ([<span style="color:blue">**Signup Sheet**</span>](https://docs.google.com/spreadsheets/d/1MnapwDyIdG9Z5LyQ0QwojV85L4lYirVd_Y6CPmFFjmU/edit?usp=sharing)): In each lecture starting from the 4th lecture (1/26), a group of 2-3 students will be tasked to present 3 papers selected for the topic they signed up for. The primary objective is to impart knowledge to the rest of the class. The presentation duration is strictly limited to 60 minutes, followed by a 10-minute question-and-answer session with the audience. The presentation will be assessed based on the following criteria (everyone from the same presentation group receives the same score):
     * Clarity: Whether the presentation effectively communicates the core concepts and insights contained in the papers.
     * Completeness: Whether the presentation adequately covers the essential messages in the 3 assigned papers within the allocated timeframe.
     * Teamwork: Whether the presentation is prepared and delivered by all team members.
     * Question answering: Whether the presenters effectively answer the questions raised by the audience.

  **Tips for presentation preparation**: It is not necessary to cover every detail of the papers (e.g., if a paper includes lengthy appendices, it is not necessary to discuss them in depth); rather, emphasis should be placed on conveying general ideas and insights: For theoretical papers, you don't need to go over each proof in detail, but need to explain the major conclusions/insights of the theoretical analysis. For empirical papers, you don't need to present every piece of experiment results, but need to articulate how the empirical findings support the major claims. A good presentation should highlight
     * The major contributions of the paper
     * Why these contributions are deemed important (e.g., did they reveal any previously unknown facts or change people's opinions on a widely acknowledged phenomenon?)
     * The most important technical details (e.g., the motivation behind a new training objective/model architecture design and the corresponding implementation)
     * The limitations of the work and how they might be addressed in the future
  
  **Deadline for slides submission**: Send your slides to the instructor and TAs via email at least 48 hours before your presentation (e.g., if presenting on Monday, slides should be submitted by Saturday 2:00 pm). You will receive feedback from the instructor to improve your slides, and if necessary, the instructor may schedule a meeting with your team to go over the slides. Late submissions after the deadline will result in a 50% presentation grade deduction.
* **Participation (20%)**: For each lecture starting from the 4th lecture (1/26), everyone is required to complete the following two mini-assignments (regardless of whether or not you have signed up to present for that lecture):
     * Pre-lecture question: Your task is to read the 3 papers to be introduced in the lecture, and submit a question you have when you read them. The question must not be trivial (e.g., "Does the proposed method work?" / "What is the aim of the paper?"). You are also welcome to raise these questions in the 10-minute question-and-answer session during class. **The deadline is one day before the lecture** (e.g., For Monday lectures, you need to submit the question by Sunday 11:59 pm).
     * Post-lecture feedback: After attending the lecture, you are required to provide feedback to the presenters. You should comment on the clarity, completeness, depth, etc. Your feedback doesn't have to be long, but should be specific and constructive. The deadline is each Friday (**both Monday & Wednesday feedback is due Friday 11:59 pm**).
* **Project (50%)**: By the end of this course, you are required to complete a research project, present your results, and submit a project report. You are required to work in a team of 2 or 3 students (any team size that deviates from this range requires prior approval from the instructor). There are two acceptable project types:
     * A comprehensive survey report: The survey should carefully examine and summarize existing literature on a topic covered in this course, and provide detailed and insightful discussions on the unresolved issues, challenges, and potential future opportunities within the chosen topic.
     * A hands-on project: The project is not constrained to the course topics but must be centered around NLP. The project does not have to involve large language models either. For example, you may choose to train or analyze smaller-scale language models for specific tasks. You are eligible to receive extra credits if the final project reaches a publishable state.

  The project grading breakdown (50%) is as follows:
     * Project proposal ([<span style="color:blue">**Guideline**</span>](https://docs.google.com/document/d/1bRtEN06gzBD5fS_33VE7RVYftdpQE6cauoHz5tTt6cc/edit?usp=sharing)): 5% (**Deadline**: 2/6)
     * Mid-term report ([<span style="color:blue">**Guideline**</span>]()): 10% (**Deadline**: 3/13)
     * Final presentation (**Deadline**: 4/20; [<span style="color:blue">**Guideline**</span>]()) and final report (**Deadline**: 5/5; [<span style="color:blue">**Guideline**</span>]()): 35% 

## Schedule <span style="font-size: 0.8em; font-weight: normal; color: #666;">(subject to changes)</span>

<style>
  /* Scoped styling for this page's schedule table */
  .course-page {
    line-height: 1.7;
    font-size: 1rem;
  }
  .course-page h2 { margin-top: 2rem; margin-bottom: 0.75rem; }
  .course-page h3 { margin-top: 1.5rem; margin-bottom: 0.5rem; }
  .course-page p { margin: 0.4rem 0 0.9rem; }
  .course-page ul, .course-page ol { margin: 0.4rem 0 1rem 1.25rem; }
  .course-page li { margin: 0.25rem 0; }
  .course-page .schedule { margin-top: 0.75rem; }

  .schedule table {
    width: 100%;
    border-collapse: separate;
    border-spacing: 0;
    font-size: 0.95rem;
    background: #ffffff;
    border: 1px solid #e6e8eb;
    border-radius: 8px;
    overflow: hidden;
  }
  .schedule th, .schedule td {
    padding: 10px 12px;
    border-bottom: 1px solid #eef1f4;
    vertical-align: top;
  }
  .schedule th {
    background: #f7f9fc;
    color: #111827;
    font-weight: 600;
    text-align: left;
  }
  .schedule tr:nth-child(even) td { background: #fbfcfe; }
  .schedule tr:hover td { background: #f5f9ff; }
  .schedule .muted { color: #6b7280; font-size: 0.85em; white-space: nowrap; }
  .schedule .notice {
    font-size: 1.05rem;
    color: #1d4ed8; /* highlighted indigo */
    background: #eef2ff;
    border-left: 4px solid #6366f1;
    padding: 8px 12px;
    margin: 6px 0 10px 0;
    border-radius: 4px;
  }
  .schedule .guest {
    display: inline-block;
  font-size: inherit;
    font-weight: 600;
    color: #7a2e0e;
    background: #fff7ed; /* warm highlight */
    border: 1px solid #f1c084;
    padding: 3px 10px;
    border-radius: 14px;
  }
  .schedule .tag {
    display: inline-block; padding: 2px 8px; font-size: 0.85em; border-radius: 12px;
    border: 1px solid #b2e3c4; background: #e9f9ef; color: #0f8a43; white-space: nowrap;
  }
  .schedule .nowrap { white-space: nowrap; }
  
  /* Added to support section headers from 2026 file */
  .section-header {
    background-color: #f0f4f8;
    text-align: center;
    font-weight: 600;
    padding: 12px;
  }
</style>

<div class="schedule" markdown="1">

<table >
  <thead>
    <tr>
      <th>Date</th>
      <th>Topic</th>
      <th>Papers</th>
      <th>Slides</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td colspan="4" class="section-header">Introduction to Large Language Models</td>
    </tr>
    <tr>
      <td >1/12</td>
      <td >Course Overview</td>
      <td>-</td>
      <td><a href="/teaching/2026-spring-cs6770/overview.pdf">overview</a></td>
    </tr>
    <tr>
      <td >1/14</td>
      <td >Language Model Architecture</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality (word2vec)</a></li>
          <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (Transformer)</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/lm_basics.pdf">lm_basics</a></td>
    </tr>
    <tr>
      <td >1/19</td>
      <td ><span class="tag">No Class (MLK Holiday)</span></td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td >1/21</td>
      <td >Language Model Pretraining & Fine-Tuning</td>
      <td><ul class="paper-list">
          <li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners (GPT-2)</a></li>
          <li><a href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li>
          <li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
          <li><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/pretrain_finetune.pdf">pretrain_finetune</a></td>
    </tr>
    <tr>
      <td >1/26</td>
      <td >In-Context Learning</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2111.02080">An Explanation of In-context Learning as Implicit Bayesian Inference</a></li>
          <li><a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a></li>
          <li><a href="https://arxiv.org/abs/2404.11018">Many-Shot In-Context Learning</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/icl.pdf">icl</a></td>
    </tr>
    <tr>
      <td >1/28</td>
      <td >Scaling and Emergent Ability</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/scaling.pdf">scaling</a></td>
    </tr>
<tr>
    <td colspan="4" class="section-header">Reasoning with Language Models</td>
  </tr>    
    <tr>
      <td >2/2</td>
      <td >Chain-of-Thought Prompting</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2205.10625">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/cot.pdf">cot</a></td>
    </tr>
    <tr>
      <td >2/4</td>
      <td >Inference-Time Scaling</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2407.21787">Large Language Monkeys: Scaling Inference Compute with Repeated Sampling</a></li>
          <li><a href="https://arxiv.org/abs/2408.03314">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/inf_scale.pdf">inf_scale</a></td>
    </tr>
    <tr>
      <td >2/9</td>
      <td >Reinforcement Learning Algorithms for LLM Reasoning</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2402.03300">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2503.14476">DAPO: An Open-Source LLM Reinforcement Learning System at Scale</a></li>
          <li><a href="https://arxiv.org/abs/2503.20783">Understanding R1-Zero-Like Training: A Critical Perspective</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/rl_alg.pdf">rl_alg</a></td>
    </tr>
    <tr>
      <td >2/11</td>
      <td >Reinforcement Learning with Verifiable Rewards</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
          <li><a href="https://arxiv.org/abs/2504.13837">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a></li>
          <li><a href="https://arxiv.org/abs/2505.24864">ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/rlvr.pdf">rlvr</a></td>
    </tr>
<tr>
    <td colspan="4" class="section-header">Knowledge & Retrieval-Augmented Generation</td>
  </tr>    
    <tr>
      <td >2/16</td>
      <td >Parametric Knowledge in Language Models</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/1909.01066">Language Models as Knowledge Bases?</a></li>
          <li><a href="https://arxiv.org/abs/2012.14913">Transformer Feed-Forward Layers Are Key-Value Memories</a></li>
          <li><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/knowledge.pdf">knowledge</a></td>
    </tr>
    <tr>
      <td >2/18</td>
      <td >Retrieval-Augmented Language Generation (RAG)</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/1911.00172">Generalization through Memorization: Nearest Neighbor Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
          <li><a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/rag.pdf">rag</a></td>
    </tr>
    <tr>
      <td >2/23</td>
      <td >Long Context Language Models</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a></li>
          <li><a href="https://arxiv.org/abs/2401.01325">LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</a></li>
          <li><a href="https://arxiv.org/abs/2310.03025">Retrieval meets Long Context Large Language Models</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/long_context.pdf">long_context</a></td>
    </tr>
<tr>
    <td colspan="4" class="section-header">Efficiency</td>
  </tr>    
    <tr>
      <td >2/25</td>
      <td >Efficient Model Architectures</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></li>
          <li><a href="https://arxiv.org/abs/2309.17453">Efficient Streaming Language Models with Attention Sinks</a></li>
          <li><a href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/arch.pdf">arch</a></td>
    </tr>
    <tr>
      <td >3/2</td>
      <td ><span class="tag">No Class (Spring Recess)</span></td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td >3/4</td>
      <td ><span class="tag">No Class (Spring Recess)</span></td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td >3/9</td>
      <td >Decoding Acceleration</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding</a></li>
          <li><a href="https://arxiv.org/abs/2401.10774">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></li>
          <li><a href="https://arxiv.org/abs/2401.15077">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/decoding.pdf">decoding</a></td>
    </tr>
<tr>
    <td colspan="4" class="section-header">Language Model Alignment</td>
  </tr>
    <tr>
      <td >3/11</td>
      <td >Instruction Tuning (Multi-Task)</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a></li>
          <li><a href="https://arxiv.org/abs/2104.08773">Cross-Task Generalization via Natural Language Crowdsourcing Instructions</a></li>
          <li><a href="https://arxiv.org/abs/2204.07705">Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/multi_task.pdf">multi_task</a></td>
    </tr>
    <tr>
      <td >3/16</td>
      <td >Instruction Tuning (Chat-Style)</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></li>
          <li><a href="https://arxiv.org/abs/2308.06259">Self-Alignment with Instruction Backtranslation</a></li>
          <li><a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/instruct.pdf">instruct</a></td>
    </tr>
    <tr>
      <td >3/18</td>
      <td >Reinforcement Learning from Human Feedback (RLHF)</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></li>
          <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li>
          <li><a href="https://arxiv.org/abs/2405.14734">SimPO: Simple Preference Optimization with a Reference-Free Reward</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/rlhf.pdf">rlhf</a></td>
    </tr>
<tr>
    <td colspan="4" class="section-header">Language Agents</td>
  </tr>
    <tr>
      <td >3/23</td>
      <td >Language Agent Basics</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a></li>
          <li><a href="https://arxiv.org/abs/2303.11366">Reflexion: Language Agents with Verbal Reinforcement Learning</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/agent.pdf">agent</a></td>
    </tr>
    <tr>
      <td >3/25</td>
      <td >Language Models for Code</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2204.05999">InCoder: A Generative Model for Code Infilling and Synthesis</a></li>
          <li><a href="https://arxiv.org/abs/2304.05128">Teaching Large Language Models to Self-Debug</a></li>
          <li><a href="https://arxiv.org/abs/2405.15793">SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/code_lm.pdf">code_lm</a></td>
    </tr>
    <tr>
      <td >3/30</td>
      <td >Deep Research</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2503.09516">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</a></li>
          <li><a href="https://arxiv.org/abs/2504.03160">DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments</a></li>
          <li><a href="https://arxiv.org/abs/2510.24701">Tongyi DeepResearch Technical Report</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/research.pdf">research</a></td>
    </tr>
    <tr>
      <td >4/1</td>
      <td >Multimodal Language Models</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2204.14198">Flamingo: a Visual Language Model for Few-Shot Learning</a></li>
          <li><a href="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning (LLaVA)</a></li>
          <li><a href="https://arxiv.org/abs/2501.01957">VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/multimodal.pdf">multimodal</a></td>
    </tr>
    <tr>
      <td colspan="4" class="section-header">Ethical Considerations and Evaluations of Language Models</td>
    </tr>
    <tr>
      <td >4/6</td>
      <td >Security and Jailbreaking</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2306.11698">DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models</a></li>
          <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2310.06987">Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/security.pdf">security</a></td>
    </tr>
    <tr>
      <td >4/8</td>
      <td >Memorization and Legal Issues</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2012.07805">Extracting Training Data from Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2202.07646">Quantifying Memorization Across Neural Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2601.02671">Extracting Books from Production Language Models</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/memorization.pdf">memorization</a></td>
    </tr>
    <tr>
      <td >4/13</td>
      <td >Language Models for Evaluation</td>
      <td><ul class="paper-list">
          <li><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a></li>
          <li><a href="https://arxiv.org/abs/2404.04475">Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators</a></li>
          <li><a href="https://arxiv.org/abs/2404.13076">LLM Evaluators Recognize and Favor Their Own Generations</a></li>
        </ul></td>
      <td><a href="/teaching/2026-spring-cs6770/eval.pdf">eval</a></td>
    </tr>
    <tr>
      <td >4/15</td>
      <td ><span class="tag">TBD</span></td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td >4/20</td>
      <td ><span class="tag">TBD</span></td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td >4/22</td>
      <td ><span class="tag">Project Presentations</span></td>
      <td>-</td>
      <td>-</td>
    </tr>
    <tr>
      <td >4/27</td>
      <td ><span class="tag">Project Presentations</span></td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

</div>
## Useful Materials
* For NLP background: [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)
* For deep learning background: [Deep Learning](https://www.deeplearningbook.org/)

## Students with Disabilities or Learning Needs
It is my goal to create a learning experience that is as accessible as possible. If you anticipate any issues related to the format, materials, or requirements of this course, please meet with me outside of class so we can explore potential options. Students with disabilities may also wish to work with the Student Disability Access Center (SDAC) to discuss a range of options to removing barriers in this course, including official accommodations. You may email an SDAC advisor at [cmacmasters@virginia.edu](mailto:cmacmasters@virginia.edu) to schedule an appointment. For general questions please visit the SDAC website: [sdac.studenthealth.virginia.edu](https://sdac.studenthealth.virginia.edu). If you have already been approved for accommodations through SDAC, please send me your accommodation letter and meet with me so we can develop an implementation plan together.

## Religious Accommodations
It is the University's long-standing policy and practice to reasonably accommodate students so that they do not experience an adverse academic consequence when sincerely held religious beliefs or observances conflict with academic requirements.

Students who wish to request academic accommodation for a religious observance should submit their request to me by email as far in advance as possible. Students who have questions or concerns about academic accommodations for religious observance or religious beliefs may contact the [University’s Office for Equal Opportunity and Civil Rights (EOCR)](https://eocr.virginia.edu/) at [UVAEOCR@virginia.edu](mailto:UVAEOCR@virginia.edu) or 434-924-3200.

## Harassment, Discrimination, and Interpersonal Violence
The University of Virginia is dedicated to providing a safe and equitable learning environment for all students. If you or someone you know has been affected by power-based personal violence, more information can be found on the UVA Sexual Violence website that describes reporting options and resources available - [www.virginia.edu/sexualviolence](https://www.virginia.edu/sexualviolence).  

The same resources and options for individuals who experience sexual misconduct are available for discrimination, harassment, and retaliation.  UVA prohibits discrimination and harassment based on age, color, disability, family medical or genetic information, gender identity or expression, marital status, military status, national or ethnic origin, political affiliation, pregnancy (including childbirth and related conditions), race, religion, sex, sexual orientation, or veteran status. UVA policy also prohibits retaliation for reporting such behavior. 

If you witness or are aware of someone who has experienced prohibited conduct, you are encouraged to submit a report to Just Report It ([justreportit.virginia.edu](https://justreportit.virginia.edu)) or contact EOCR, the office of Equal Opportunity and Civil Rights.

If you would prefer to disclose such conduct to a confidential resource where what you share is not reported to the University, you can turn to Counseling & Psychological Services ("CAPS") and [Women’s Center Counseling Staff and Confidential Advocates](https://womenscenter.virginia.edu/counseling/our-counseling-services) (for students of all genders).  

As your professor and as a person, know that I care about you and your well-being and stand ready to provide support and resources as I can. As a faculty member, I am a responsible employee, which means that I am required by University policy and by federal law to report certain kinds of conduct that you report to me to the University's Title IX Coordinator. The Title IX Coordinator's job is to ensure that the reporting student receives the resources and support that they need, while also determining whether further action is necessary to ensure survivor safety and the safety of the University community. 

## Student Support Team
You have many resources available to you when you experience academic or personal stresses. In addition to the course staff, the School of Engineering and Applied Science has staff members located in Thornton Hall who you can contact to help manage academic or personal challenges. Please do not wait until the end of the semester to ask for help!

## Community and Identity

The [Center for Diversity in Engineering (CDE)](https://engineering.virginia.edu/about-our-school/diversity-equity-and-engagement/center-diversity-engineering) is a student space dedicated to advocating for underrepresented groups in STEM. It exists to connect students with the academic, financial, health, and community resources they need to thrive both at UVA and in the world.  The CDE includes an open study area, event space, and staff members on site. Through this space, we affirm and empower equitable participation toward intercultural fluency and provide the resources necessary for students to be successful during their academic journey and future careers.
</div>
