---
permalink: /
title: "About Me"
author_profile: true
layout: archive
redirect_from: 
  - /about/
  - /about.html
---

I joined the Computer Science (CS) department at the University of Virginia (UVA) in 2024 as an assistant professor (tenure-track). Previously, I earned my Ph.D. from University of Illinois Urbana-Champaign (UIUC) where I worked with [Jiawei Han](http://hanj.cs.illinois.edu/) and spent time as a visiting researcher at the Princeton NLP Group, working with [Danqi Chen](https://www.cs.princeton.edu/~danqic/). 

<span style="color:blue">**I am looking for self-motivated PhD students and interns! Please fill out [this form](https://forms.gle/8DCBEuVbTFw4ARm5A) if you are interested in working with me. After completing the form, you are also welcome to reach out via email. I will read all submitted forms and emails but I do apologize for not being able to respond to each of them!**</span>

Research
======

I am broadly interested in the fields of natural language processing (NLP), machine learning (ML), and data mining. Nowadays, I am especially passionate about the developments in large language models (LLMs). Some of my past papers are as follows:  
* **Pretraining and Representation Learning for NLP**:
  * [ICLR'24 Meng et al.] [Representation Deficiency in Masked Language Modeling](https://arxiv.org/abs/2302.02060)
  * [ICLR'22 Meng et al.] [Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators](https://arxiv.org/abs/2204.03243)
  * [NeurIPS'21 Meng et al.] [COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining](https://arxiv.org/abs/2102.08473)
  * [NeurIPS'19 Meng et al.] [Spherical Text Embedding](https://arxiv.org/abs/1911.01196)
* **Large Language Models for Few-Shot and Zero-Shot Learning**:
  * [ICML'23 Meng et al.] [Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning](https://arxiv.org/abs/2211.03044)
  * [NeurIPS'22 Meng et al.] [Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](https://arxiv.org/abs/2202.04538)
* **Learning from Weak Supervision**:
  * [EMNLP'21 Meng et al.] [Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training](https://arxiv.org/abs/2109.05003)
  * [EMNLP'20 Meng et al.] [Text Classification Using Label Names Only: A Language Model Self-Training Approach](https://arxiv.org/abs/2010.07245)
  * [CIKM'18 Meng et al.] [Weakly-Supervised Neural Text Classification](https://arxiv.org/abs/1809.01478)


News
======
* \[**2024 Service**\] **ICML 2024** (Area Chair), **COLM 2024** (Area Chair), **NeurIPS 2024** (Area Chair), **ACL Rolling Review 2024** (Area Chair), **TMLR** (Action Editor).

* \[**2024.05**\] One paper on [Language Model Reasoning on Graphs](https://arxiv.org/abs/2404.07103) has been accepted by **ACL 2024 Findings**!

* \[**2024.04**\] Received the [Superalignment Fast Grants](https://openai.com/index/superalignment-fast-grants/) from OpenAI!

* \[**2024.01**\] Two papers on [Masked Language Modeling](https://arxiv.org/abs/2302.02060) and [Language Model Evaluation](https://arxiv.org/abs/2310.07641) have been accepted by **ICLR 2024**!

* \[**2023.10**\] One paper on [Weakly Supervised Text Classification](https://arxiv.org/abs/2305.13723) has been accepted by **EMNLP 2023**!

* \[**2023.09**\] One paper on [Language Models as Training Data Generators](https://arxiv.org/abs/2306.15895) has been accepted by **NeurIPS 2023 Datasets and Benchmarks Track**!

* \[**2023.05**\] One paper on [Weakly Supervised Scientific Text Classification](https://arxiv.org/abs/2306.14003) has been accepted by **KDD 2023**!

* \[**2023.05**\] Two papers on [Language Model Pretraining on Text-Rich Network](https://arxiv.org/abs/2305.12268) and [Retrieval-Enhanced Weakly-Supervised Text Classification](https://arxiv.org/abs/2305.10703) have been accepted by **ACL 2023 Main Conference/Findings**!

* \[**2023.04**\] Our tutorial on [Pretrained Language Representations for Text Understanding](https://yumeng5.github.io/kdd23-tutorial/) has been accepted by **KDD 2023**!

* \[**2023.04**\] One paper on [Few-Shot Learning](https://arxiv.org/abs/2211.03044) has been accepted by **ICML 2023**!


Education
======
* Ph.D. (2023) in Computer Science, University of Illinois Urbana-Champaign  
**Thesis**: [Efficient and Effective Learning of Text Representations](https://www.ideals.illinois.edu/items/129146)

* M.S. (2019) in Computer Science, University of Illinois Urbana-Champaign  
**Thesis**: [Weakly-Supervised Text Classification](https://www.ideals.illinois.edu/items/111979)

* B.S. (2017) in Computer Engineering, University of Illinois Urbana-Champaign  
Graduated with Highest Honor & [Bronze Tablet](https://digital.library.illinois.edu/items/592ebe50-1be8-0136-4cfa-0050569601ca-5#?c=0&m=0&s=0&cv=0&r=0&xywh=-3461%2C0%2C12837%2C5932)  

Contact
======
* Email: yumeng5\[at\]virginia\[dot\]edu

* Office: Rice Hall 408, 85 Engineer's Way, Charlottesville, Virginia 22093
