---
permalink: /
title: "About Me"
author_profile: true
layout: archive
redirect_from: 
  - /about/
  - /about.html
---

I joined the Computer Science (CS) department at the University of Virginia (UVA) in 2024 as an assistant professor (tenure-track). Previously, I earned my Ph.D. from University of Illinois Urbana-Champaign (UIUC) where I worked with [Jiawei Han](http://hanj.cs.illinois.edu/) and spent time as a visiting researcher at the Princeton NLP Group, working with [Danqi Chen](https://www.cs.princeton.edu/~danqic/). 

<span style="color:blue">**I am looking for self-motivated PhD students and interns! Please fill out [this form](https://forms.gle/ZsEyYewLogXodujc7) if you are interested in working with me. After completing the form, you are also welcome to reach out via email. I will read all submitted forms and emails but I do apologize for not being able to respond to each of them!**</span>

Research
======

I am broadly interested in the fields of natural language processing (NLP), machine learning (ML), and data mining. Currently, I am especially passionate about the advancements in large language models (LLMs). Here are some papers that reflect my interests:
* **Training Language Models for Better Alignment and Reliability**:
  * [NeurIPS'24 Meng et al.] [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/abs/2405.14734)
  * [ICLR'25 [Wei](https://weizhepei.com/) et al.] [InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/abs/2406.13629)
* **Large Language Models for Synthetic Data Generation**:
  * [ICML'23 Meng et al.] [Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning](https://arxiv.org/abs/2211.03044)
  * [NeurIPS'22 Meng et al.] [Generating Training Data with Language Models: Towards Zero-Shot Language Understanding](https://arxiv.org/abs/2202.04538)
* **Learning from Weak Supervision**:
  * [EMNLP'21 Meng et al.] [Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training](https://arxiv.org/abs/2109.05003)
  * [EMNLP'20 Meng et al.] [Text Classification Using Label Names Only: A Language Model Self-Training Approach](https://arxiv.org/abs/2010.07245)
  * [CIKM'18 Meng et al.] [Weakly-Supervised Neural Text Classification](https://arxiv.org/abs/1809.01478)  
* In the past, I have also worked on pretraining and representation Learning for NLP:
  * [ICLR'24 Meng et al.] [Representation Deficiency in Masked Language Modeling](https://arxiv.org/abs/2302.02060)
  * [ICLR'22 Meng et al.] [Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators](https://arxiv.org/abs/2204.03243)
  * [NeurIPS'21 Meng et al.] [COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining](https://arxiv.org/abs/2102.08473)
  * [NeurIPS'19 Meng et al.] [Spherical Text Embedding](https://arxiv.org/abs/1911.01196)



News
======

* \[**2025 Service**\] Area Chair: **ICLR, ICML, COLM, NeurIPS**. Action Editor: **TMLR**.

* \[**2025.01**\] One paper on Retrieval-Augmented Generation ([InstructRAG](https://arxiv.org/abs/2406.13629)) accepted to **ICLR 2025**!

* \[**2024.09**\] Two papers on Preference Optimization ([SimPO](https://arxiv.org/abs/2405.14734)) and Contrastive Decoding in MoE ([SCMoE](https://arxiv.org/abs/2405.14507)) accepted to **NeurIPS 2024**!

* \[**2024.09**\] Two papers on [Zero-Shot Relation Extraction](https://arxiv.org/abs/2402.11142) and [LLM Persona Survey](https://arxiv.org/abs/2406.01171) accepted to **EMNLP 2024 Main Conference/Findings**!

* \[**2024.08**\] My Ph.D. thesis won the [ACM SIGKDD 2024 Dissertation Award](https://kdd2024.kdd.org/awards/)!

* \[**2024.05**\] One paper on [Language Model Reasoning on Graphs](https://arxiv.org/abs/2404.07103) accepted to **ACL 2024 Findings**!

* \[**2024.04**\] Received the [Superalignment Fast Grants](https://openai.com/index/superalignment-fast-grants/) from **OpenAI** [(UVA Press Release)](https://engineering.virginia.edu/news-events/news/uva-professor-receives-openai-grant-inform-next-generation-ai-systems)!

* \[**2024.01**\] Two papers on [Masked Language Modeling](https://arxiv.org/abs/2302.02060) and [Language Model Evaluation](https://arxiv.org/abs/2310.07641) accepted to **ICLR 2024**!

Education
======
* Ph.D. (2023) in Computer Science, University of Illinois Urbana-Champaign  
**Thesis**: [Efficient and Effective Learning of Text Representations](https://www.ideals.illinois.edu/items/129146)
(**[ACM SIGKDD 2024 Dissertation Award](https://kdd2024.kdd.org/awards/)**)

* M.S. (2019) in Computer Science, University of Illinois Urbana-Champaign  
**Thesis**: [Weakly-Supervised Text Classification](https://www.ideals.illinois.edu/items/111979)

* B.S. (2017) in Computer Engineering, University of Illinois Urbana-Champaign  
Graduated with Highest Honor & [Bronze Tablet](https://digital.library.illinois.edu/items/592ebe50-1be8-0136-4cfa-0050569601ca-5#?c=0&m=0&s=0&cv=0&r=0&xywh=-3461%2C0%2C12837%2C5932)  

Contact
======
* Email: yumeng5\[at\]virginia\[dot\]edu

* Office: Rice Hall 408, 85 Engineer's Way, Charlottesville, Virginia 22903
